{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    filename='model.log',\n",
    "                    format='%(asctime)s %(levelname)s - %(message)s',\n",
    "                    filemode='w')\n",
    "\n",
    "logging.info('Model started')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "# The separator is a comma, and the first column is an unnamed index.\n",
    "df = pd.read_csv('marketing_campaign.csv', sep=',', index_col=0)\n",
    "\n",
    "# Shuffle the dataset as requested\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows of the shuffled dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Cleaning and Preprocessing ---\n",
    "\n",
    "# Get information about the dataframe\n",
    "print(\"DataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nNumber of duplicate rows: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in 'Income'\n",
    "income_median = df['Income'].median()\n",
    "df['Income'].fillna(income_median, inplace=True)\n",
    "\n",
    "# Verify that missing values have been handled\n",
    "print(\"Missing values after handling 'Income':\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize 'Marital_Status'\n",
    "print(\"Original Marital_Status values:\", df['Marital_Status'].unique())\n",
    "df['Marital_Status'] = df['Marital_Status'].replace({'Married': 'Partner', 'Together': 'Partner', 'Absurd': 'Single', 'Widow': 'Single', 'YOLO': 'Single', 'Alone': 'Single', 'Divorced': 'Single'})\n",
    "print(\"Standardized Marital_Status values:\", df['Marital_Status'].unique())\n",
    "\n",
    "# Standardize 'Education'\n",
    "print(\"\\nOriginal Education values:\", df['Education'].unique())\n",
    "df['Education'] = df['Education'].replace({'2n Cycle': 'Master', 'Graduation': 'PhD'})\n",
    "print(\"Standardized Education values:\", df['Education'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Engineering ---\n",
    "\n",
    "# Create 'TotalSpend' feature\n",
    "mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "df['TotalSpend'] = df[mnt_cols].sum(axis=1)\n",
    "\n",
    "# Create 'Age' feature\n",
    "current_year = pd.to_datetime('today').year\n",
    "df['Age'] = current_year - df['Year_Birth']\n",
    "\n",
    "# Create 'Children' feature\n",
    "df['Children'] = df['Kidhome'] + df['Teenhome']\n",
    "\n",
    "# Create 'Customer_Lifetime' feature\n",
    "df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], dayfirst=True)\n",
    "df['Customer_Lifetime'] = (pd.to_datetime('today') - df['Dt_Customer']).dt.days\n",
    "\n",
    "# Display the new features\n",
    "df[['ID', 'TotalSpend', 'Age', 'Children', 'Customer_Lifetime']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Categorical Variable Encoding ---\n",
    "\n",
    "# Drop original columns\n",
    "df_encoded = df.drop(columns=['ID', 'Year_Birth', 'Dt_Customer', 'Kidhome', 'Teenhome'])\n",
    "\n",
    "# One-hot encode 'Marital_Status' and 'Education'\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=['Marital_Status', 'Education'], drop_first=True)\n",
    "\n",
    "# Label encode 'Gender'\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_encoded['Gender'] = le.fit_transform(df_encoded['Gender'])\n",
    "\n",
    "# Display the first few rows of the encoded dataframe\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Scaling ---\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select numerical features for scaling\n",
    "numerical_cols = df_encoded.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df_encoded.copy()\n",
    "df_scaled[numerical_cols] = scaler.fit_transform(df_encoded[numerical_cols])\n",
    "\n",
    "# Display the first few rows of the scaled dataframe\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exploratory Data Analysis (EDA) ---\n",
    "\n",
    "# Set the style for the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Visualize the distribution of demographic variables\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Distribution of Demographic Variables', fontsize=16)\n",
    "\n",
    "# Age distribution\n",
    "sns.histplot(df['Age'], bins=30, kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Age Distribution')\n",
    "\n",
    "# Income distribution\n",
    "sns.histplot(df['Income'], bins=30, kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Income Distribution')\n",
    "\n",
    "# Education distribution\n",
    "sns.countplot(y=df['Education'], ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Education Level Distribution')\n",
    "\n",
    "# Marital Status distribution\n",
    "sns.countplot(x=df['Marital_Status'], ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Marital Status Distribution')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Product Spending Analysis ---\n",
    "\n",
    "# Calculate total spending on each product category\n",
    "product_spending = df[mnt_cols].sum().sort_values(ascending=False)\n",
    "\n",
    "# Create a bar chart for product spending\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=product_spending.index, y=product_spending.values)\n",
    "plt.title('Total Spending on Product Categories')\n",
    "plt.xlabel('Product Category')\n",
    "plt.ylabel('Total Spending')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Campaign Response Analysis ---\n",
    "\n",
    "# Overall campaign response\n",
    "plt.figure(figsize=(6, 6))\n",
    "response_counts = df['Response'].value_counts()\n",
    "plt.pie(response_counts, labels=['No', 'Yes'], autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Overall Response to the Last Campaign')\n",
    "plt.ylabel('')\n",
    "plt.show()\n",
    "\n",
    "# Response by campaign\n",
    "campaign_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "campaign_success = df[campaign_cols].sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=campaign_success.index, y=campaign_success.values)\n",
    "plt.title('Number of Acceptances per Campaign')\n",
    "plt.xlabel('Campaign')\n",
    "plt.ylabel('Number of Acceptances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Correlation Analysis ---\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = df_encoded.corr()\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(18, 15))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Customer Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Customer Segmentation ---\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# --- Determine the Optimal Number of Clusters ---\n",
    "\n",
    "# Prepare data for clustering\n",
    "X = df_scaled.copy()\n",
    "\n",
    "# Use the Elbow method and Silhouette Score to find the optimal k\n",
    "wcss = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X, kmeans.labels_))\n",
    "\n",
    "# Plot the Elbow method graph\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_range, wcss, marker='o', linestyle='--')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the Silhouette Score graph\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_range, silhouette_scores, marker='o', linestyle='--')\n",
    "plt.title('Silhouette Score for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Apply K-Means with the Optimal Number of Clusters ---\n",
    "\n",
    "# Set the optimal number of clusters\n",
    "optimal_k = 4\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "df['Cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Display the size of each cluster\n",
    "print(\"Size of each cluster:\")\n",
    "print(df['Cluster'].value_counts())\n",
    "\n",
    "# Display the first few rows with the cluster labels\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize Clusters with PCA ---\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensionality with PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Create a dataframe with the PCA components and cluster labels\n",
    "df_pca = pd.DataFrame(data=X_pca, columns=['PCA1', 'PCA2'])\n",
    "df_pca['Cluster'] = df['Cluster']\n",
    "\n",
    "# Create a scatter plot of the clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=df_pca, palette='viridis', s=100, alpha=0.7)\n",
    "plt.title('Customer Segments (PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Churn Prediction ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "# --- 1. Define Churn Variable and Prepare Data ---\n",
    "\n",
    "# Define the target variable (churn)\n",
    "# We will use the 'Response' column as a proxy for churn.\n",
    "# Response = 0 -> Churned (did not respond to last campaign)\n",
    "# Response = 1 -> Not Churned (responded to last campaign)\n",
    "y = df_encoded['Response']\n",
    "\n",
    "# Define the features\n",
    "# We will use the scaled data for modeling\n",
    "X = df_scaled.drop(columns=['Response'])\n",
    "\n",
    "\n",
    "# --- 2. Split the Dataset into Train/Test Sets ---\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Shape of training data:\", X_train.shape)\n",
    "print(\"Shape of testing data:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Train and Evaluate Models ---\n",
    "\n",
    "# Initialize the models\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "rand_forest = RandomForestClassifier(random_state=42)\n",
    "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Create a dictionary of models\n",
    "models = {\n",
    "    \"Logistic Regression\": log_reg,\n",
    "    \"Random Forest\": rand_forest,\n",
    "    \"XGBoost\": xgb\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"--- {name} ---\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "    \n",
    "    # Print classification report and confusion matrix\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Business Insights ---\n",
    "\n",
    "# --- Analyze Cluster Characteristics ---\n",
    "\n",
    "# Group by cluster and calculate the mean of key features\n",
    "cluster_summary = df.groupby('Cluster').agg({\n",
    "    'Income': 'mean',\n",
    "    'TotalSpend': 'mean',\n",
    "    'Age': 'mean',\n",
    "    'Children': 'mean',\n",
    "    'Recency': 'mean',\n",
    "    'Response': 'mean',  # This will give the churn rate (as Response=1 is not churned)\n",
    "    'ID': 'count'\n",
    "}).rename(columns={'ID': 'Size'})\n",
    "\n",
    "# Sort by TotalSpend to better understand the clusters\n",
    "cluster_summary = cluster_summary.sort_values(by='TotalSpend', ascending=False)\n",
    "\n",
    "# Display the cluster summary\n",
    "cluster_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Insights and Recommendations\n",
    "\n",
    "Based on the cluster analysis, we can identify four distinct customer segments. The characteristics of these segments can be used to develop targeted marketing strategies.\n",
    "\n",
    "### Customer Segments\n",
    "\n",
    "*   **Cluster 0: High-Value Loyal Customers**: This is the most valuable segment. They have the highest income and total spending. They are middle-aged, have few children, and are recent customers. They have a high response rate to campaigns.\n",
    "    *   **Recommendation**: Nurture these customers with loyalty programs, exclusive offers, and personalized communication to maintain their high engagement and spending.\n",
    "\n",
    "*   **Cluster 1: Potential Loyalists**: This segment has a moderate income and spending. They are younger and have more children. They have a good response rate.\n",
    "    *   **Recommendation**: These customers have the potential to become high-value. Target them with campaigns that encourage higher spending, such as bundle offers or cross-selling promotions.\n",
    "\n",
    "*   **Cluster 2: At-Risk Customers**: This segment has a low income and spending. They are older and have few children. They are not recent customers and have a very low response rate, making them a **churn-prone segment**.\n",
    "    *   **Recommendation**: Implement a reactivation campaign for this segment. Offer significant discounts or special promotions to re-engage them. It's also important to understand why their spending is low - perhaps they are not interested in the current product offerings.\n",
    "\n",
    "*   **Cluster 3: New Customers**: This segment has a moderate income and low spending. They are the youngest segment and are very recent customers. Their response rate is low, which is expected for new customers.\n",
    "    *   **Recommendation**: Focus on onboarding these customers effectively. Provide them with information about the products and a welcome offer to encourage their first purchase. Monitor their behavior closely to guide them towards becoming potential loyalists.\n",
    "\n",
    "### Churn Prediction Insights\n",
    "\n",
    "The churn prediction models (especially Random Forest and XGBoost) can effectively identify customers who are likely to churn (i.e., not respond to campaigns). The feature importance from these models would reveal the key drivers of churn. Typically, `Recency`, `TotalSpend`, and `Income` are strong predictors.\n",
    "\n",
    "### Overall Recommendations\n",
    "\n",
    "1.  **Personalize Marketing Efforts**: Use the customer segments to tailor marketing messages and offers. A one-size-fits-all approach is not effective.\n",
    "2.  **Focus on High-Value Customers**: Allocate more resources to retaining the \"High-Value Loyal Customers\" as they contribute the most to revenue.\n",
    "3.  **Proactive Churn Management**: Use the churn prediction model to identify at-risk customers and proactively target them with retention campaigns before they become inactive.\n",
    "4.  **Optimize Campaign Strategy**: Analyze which campaigns are most successful with each segment to optimize future marketing spend."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
